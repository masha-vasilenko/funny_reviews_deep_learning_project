{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport string\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n'''from nltk.tokenize import WordPunctTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n'''\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"4c8b0a68b09ee8fecbbfccdbeb300b63205e542d","trusted":true},"cell_type":"code","source":"yelp_business = pd.read_json('../data/yelp_dataset/business.json', lines=True)\nyelp_business.fillna('NA', inplace=True)\n# we want to make sure we only work with restaurants -- nothing else\nrestaurants = yelp_business[yelp_business['categories'].str.contains('Restaurants')]\nprint('Number of all businesses: ',yelp_business.shape[0])\nprint(f\"Shape of restaurants dataset{restaurants.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yelp_business.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"restaurants.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a6fee5c004788aaff55656025270d0adf7c3747"},"cell_type":"markdown","source":"Now we bring the reviews and perform some preprocessing on those reviews.."},{"metadata":{"_uuid":"508d28c7fdb9065e3dbaf0a39472297fc3b292cf","trusted":false},"cell_type":"code","source":"yelp_review_iter = pd.read_json('../data/yelp_dataset/review.json', chunksize=100000, lines=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6689f58f21411bac18026ba6edfbd0248a8b2f8"},"cell_type":"markdown","source":"Because reviews are too big, we will read them in chunks, and make sure we delete reviews of places that are not in our list of businesses filtered earlier. Note here we choose 5 chunks, but we could have chosen any number (larger numbers will give MemoryError later on)."},{"metadata":{"_uuid":"70007e05b3f8828eb82cc4c4f926f8e657861adb","trusted":false},"cell_type":"code","source":"yelp_review = pd.DataFrame()\ni=0\nfor df in yelp_review_iter:\n    \n    df = df[df['business_id'].isin(restaurants['business_id'])]\n    print(df.shape)\n    yelp_review = pd.concat([yelp_review, df])\n    i=i+1\n    print(i)\n    if i==70: break","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"yelp_review.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85169d7858504ee719c36e795ba46f8eb9a3c909"},"cell_type":"markdown","source":"Also make sure we only get businesses that already show up in our review list and delete the rest."},{"metadata":{"trusted":false},"cell_type":"code","source":"import pickle\nyelp_review.to_pickle(\"pickled_reviews.pickle\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews =pd.read_pickle(\"pickled_reviews.pickle\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1aad8f5fe22db8c3940e07fc3ae49d94000725bf","trusted":false},"cell_type":"code","source":"yelp_business = yelp_business[yelp_business['business_id'].isin(rest_reviews['business_id'])]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6385a3d6f0f21f722925451ecf38ff5c90647b21","trusted":false},"cell_type":"code","source":"print('Final businesses shape: ', yelp_business.shape)\nprint('Final review shape: ', rest_reviews.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews['funny'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.options.display.max_seq_items = 2000\nprint(rest_reviews[rest_reviews['funny']==1290][['business_id', 'text']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews.loc[1331304,'text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check:\nprint( (rest_reviews['funny']>4).mean())\nprint(f\"Number of funny reviews:{(rest_reviews['funny']>4).sum()}\")\n#print(rest_reviews['fun_bin'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"75269/4201684","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews['fun_bin']=rest_reviews['funny'].apply(lambda x: 1 if x>4 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(rest_reviews['fun_bin'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Getting a df with funny reviews"},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_fun = rest_reviews[rest_reviews['fun_bin']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_fun.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_fun.drop_duplicates(subset= 'text', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_fun.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sampling not funny reviews\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_not_fun = rest_reviews[rest_reviews['fun_bin']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"idx = rest_reviews_not_fun.index.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Balancing the classes: getting the same number of not funny reviews as funny\n#random_hotels = np.random.choice(neg_activity_df[\"hotel\"].unique(), len(neg_activity_df))\nrandom_idx = np.random.choice(idx,rest_reviews_fun.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(random_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_not_fun = rest_reviews_not_fun.loc[random_idx,:].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_not_fun.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"reviews_final = pd.concat([rest_reviews_fun, rest_reviews_not_fun])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"reviews_final.to_csv(\"../data/yelp_dataset/balanced_reviews.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the reviews[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_final = pd.read_csv(\"../input/yelp-reviews/balanced_reviews.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_final.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   Unnamed: 0             business_id   ...                   user_id fun_bin\n0          17  cHdJXLlKNWixBXpDwEGb_A   ...    JSrP-dUmLlwZiI7Dp3PQ2A       1\n1          21  Mem13A3C202RzT53npn4NA   ...    5JVY32_bmTBfIGpCCsnAfw       1\n2          62  SU56w479vUfFHsvmvQIf7A   ...    DoRCeCcJbrsM2BiAKj3trA       1\n3         126  tjAeaGdxf7I4xN9M7wGJNQ   ...    x37OyP--VEFE5p-xreplYA       1\n4         246  FhIeCF6QrsLaRvAeu0oEPQ   ...    2k8OVAPxlXHsA5X6EIoQpQ       1\n\n[5 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>business_id</th>\n      <th>cool</th>\n      <th>date</th>\n      <th>funny</th>\n      <th>review_id</th>\n      <th>stars</th>\n      <th>text</th>\n      <th>useful</th>\n      <th>user_id</th>\n      <th>fun_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17</td>\n      <td>cHdJXLlKNWixBXpDwEGb_A</td>\n      <td>1</td>\n      <td>2015-04-01 16:30:00</td>\n      <td>7</td>\n      <td>6BnQwlxRn7ZuWdzninM9sQ</td>\n      <td>3</td>\n      <td>I love chinese food and I love mexican food. W...</td>\n      <td>1</td>\n      <td>JSrP-dUmLlwZiI7Dp3PQ2A</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>21</td>\n      <td>Mem13A3C202RzT53npn4NA</td>\n      <td>9</td>\n      <td>2017-05-13 10:41:43</td>\n      <td>6</td>\n      <td>IPw8yWiyqnfBzzWmypUHgg</td>\n      <td>5</td>\n      <td>If you are looking for the best pierogies in P...</td>\n      <td>9</td>\n      <td>5JVY32_bmTBfIGpCCsnAfw</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>62</td>\n      <td>SU56w479vUfFHsvmvQIf7A</td>\n      <td>6</td>\n      <td>2016-07-25 03:55:20</td>\n      <td>5</td>\n      <td>E4LqIZ7DJd_R4ZHSNKx4RQ</td>\n      <td>4</td>\n      <td>So good! They didn't make it to 5 stars due to...</td>\n      <td>7</td>\n      <td>DoRCeCcJbrsM2BiAKj3trA</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>126</td>\n      <td>tjAeaGdxf7I4xN9M7wGJNQ</td>\n      <td>4</td>\n      <td>2014-07-13 14:32:56</td>\n      <td>5</td>\n      <td>TaoaX7MqCujFRNaJBns2Sw</td>\n      <td>5</td>\n      <td>While the prices are a bit high for a make-you...</td>\n      <td>8</td>\n      <td>x37OyP--VEFE5p-xreplYA</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>246</td>\n      <td>FhIeCF6QrsLaRvAeu0oEPQ</td>\n      <td>4</td>\n      <td>2013-06-24 06:42:29</td>\n      <td>5</td>\n      <td>3Qc49B7dA0ONmCxrn5iwCQ</td>\n      <td>2</td>\n      <td>OVERALL: The food isn't good (I explain below)...</td>\n      <td>13</td>\n      <td>2k8OVAPxlXHsA5X6EIoQpQ</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reviews = reviews_final[['funny','text', 'fun_bin']]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)\ndf_reviews.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"   funny   ...   fun_bin\n0  7       ...    1     \n1  6       ...    1     \n2  5       ...    1     \n3  5       ...    1     \n4  5       ...    1     \n\n[5 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>funny</th>\n      <th>text</th>\n      <th>fun_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>I love chinese food and I love mexican food. What can go wrong? A couple of things. First things first, this place is more of a \"rice bowl\" kind of place. I thought it was going to be more diverse as far as the menu goes, but its mainly rice bowls you get with different kinds of meats. The ordering was a little confusing at first, but one of the employees helped us out and I got the 2-item bowl and got the jade chicken and hengrenade chicken with all rice(jerk). I also ordered a jade chicken quesadilla on the side.\\n\\nI'm gonna admit, this place looks kinda dirty. I don't think Arizona uses those health department letter grade system like California does, but if I were to just judge by how it looked inside, i'd give it a \"C\" grade lol. We waited for about 15 minutes or so and finally got our food. We took it to go and ate at our hotel room. \\n\\nMmmm... the food was just alright. The jade chicken was nothing special. It tasted like any generic chinese fast food orange chicken/sesame chicken variant. The hengrenade chicken, although was the less spicier version of the jerk chicken, was still pretty spicy for me. Just be warned the jerk chicken is super spicy. If you aren't sure, ask for a sample at the restaurant before ordering, but it was way too spicy for me. \\n\\nThe jade chicken quesadilla was decent, but nothing special. Just imagine orange chicken in between a tortilla and cheese. A friend of mine ordered a jade chicken burrito and we were confused when we pulled it out of the bag because it was literally the size of Mcdonald's apple pie. If you order the burrito, be warned that it's a burrito for gnomes and smurfs, but he said it was tasty. \\n\\nThey provide a snicker doodle sugar cookie for each meal and it was decent, again nothing special. \\n\\nNot gonna lie, the next day my stomach felt like a little mexican dude and chinese dude were wrestling and throwing molotov cocktails inside. I used the bathroom like 5 times. I don't recommend eating this place if you have a lot to do the next day.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>If you are looking for the best pierogies in Pittsburgh, this is your place. There are a few small tables outside but most of the business is carry out. Pierogies Plus wins Best Pierogies every year. Why? Because the owner is from Poland and she is making the real deal pierogies. The best part is that they are hand pinched by a group of older Polish and Hungarian women. \\nThe biggest seller is potato and cheese but they sell many flavors. They are like plump pillows of softness. You can buy them buy the dozen. You can get them cold to take home and freeze or warm and ready to eat. The warm ones are served with butter and onions.  It's definitely a comfort food. The best part is that they ship internationally. Yes, they are that good.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>So good! They didn't make it to 5 stars due to the prices are a bit high for the amount of food and the location is a bit unsavory. \\nThe decor and atmosphere was surprisingly nice, from the outside I expected to be more run down inside. The staff was very nice. We were surprised how empty the dining room was for a Friday evening.\\nWe got Vegetable Samosas to start then ordered Chicken Tikka Masala, Lamb Rogan Josh, rice and plain Naan. Our only complaint was the lamb could've been more tender but everything was flavorful and delicious. \\nI would definitely go again if given the chance.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>While the prices are a bit high for a make-your-own pizza, the taste makes up for it. I love going to Seventh Street market, sitting Not Just Coffee and having a drink while waiting for delicious fresh made pizza from Pure.  I've taken this to go as well as eaten inside the market, and I can say that the pizza doesn't do well reheated. So try to eat it fresh while there if possible.\\n\\nIf one of their specialty pizzas sounds good to you, go for it, as those are definitely a better deal for the amount of toppings you get for the money.  I wanted what I wanted, though, so I ended up with a medium, thin crust, regular crust pizza with jalapenos, pepperoncini, onions and feta.  It was pretty expensive at $2/topping = $20 med pizza. But it was delicious.\\n\\nThe arugula salad with goat cheese and lemon vinaigrette is to-die-for. I crave that dressing days later. So light and fresh but flavorful.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>OVERALL: The food isn't good (I explain below), but this place may still be worth locals' time (and more importantly money).  Let me explain...\\n\\nThere are not many \"old\" restaurants in this town. We don't seem to value/frequent/patronize places that have been around putting out food for a long time. I think we should. Even when the food isn't show stopping. Why? This place has tremendous character and charm. There's an \"Old Western Vegas\" feel to Bob Taylor's. Established in 1955, it's the oldest restaurant in Las Vegas. Its a throwback to a rugged, carnivorous cowboy culture that has existed in this town for decades. And still exists. I did appreciate the slice of Vegas kitch that Bob Taylor's offers. \\n\\nFOOD ISSUES: So with all that charm how could this place go wrong? This place could be great. It really should be great. But they are not putting enough care into the food. I ordered the rib eye and asked for it to be medium rare. I was worried about it being overcooked and figured if a mistake was made, I'd be in the medium range. My instincts were correct. But the steak was closer to well done. In total, three steaks at our table were seriously overcooked. In a steakhouse. With a man tasked with grilling the steaks. Sigh. The fourth steak, smoked prime rib, was cooked properly. But the prime rib is cooked ahead if time, right? How was I prepared for this overcooked piece of meat? How did this only occasional red-meat-eater suspect that my steak would not be treated with attentive care? \\n\\nI'll tell you. When we walked in there was a large grill at the front of the restaurant with a number if steaks cooking on it. But the chef was not watching the meat. He wasn't even in front of the grill. He was nowhere to be seen as we walked through the doors. And there were at least 4 steaks cooking when we arrived. So I figured that my steak would receive the same lack of attention.  \\n\\nI ordered a simple naked potato and a side salad to accompany my steak. Both were fine. But there is not much room to mess up a potato and iceberg, is there? People rave about the garlic bread and I think it's because the rest of the meal is so mediocre, that the cheesy bread becomes the highlight if the meal. It was just OK. The most inexperienced cook could make it at home with sourdough, butter, and three types of cheese. \\n\\nA few people in our party ordered the mushroom rice side dish and it was not good. After tasting it, I was grateful to have passed on this wet, mush. \\n\\nSERVICE: Our waitress was very attentive and responsive. She was more than willing to return the overcooked steaks. \\n\\nI won't be back, but I'm glad to have visited this historic spot.\\n\\nService: 4 stars\\n\\nKitch: 4 stars\\n\\nFood: 1 star</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Data pre-processing"},{"metadata":{},"cell_type":"markdown","source":"#### Goals\n- Keep punctuation\n- Split by \".\", \"!\" to account for misspeling (like \"Hi!I went to...\")\n- Try TF-IDF?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install -U spacy[cuda92]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install spacymoji","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n#import nltk\n#from nltk.corpus import stopwords\n#from nltk.stem import SnowballStemmer\n\nfrom string import punctuation\n\n\nimport numpy as np \nimport pandas as pd \nimport os\nimport spacy\nimport string\nimport re\nimport numpy as np\nfrom spacy.symbols import ORTH\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence \n#from spacymoji import Emoji","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nre_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\ndef sub_br(x): return re_br.sub(\"\\n\", x)\n\n#nlp = spacy.load(\"en\")\nnlp = spacy.load('en_core_web_sm')\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n\ndef clean_text(text):\n    ''' Pre process and convert texts to a list of words '''\n    text = str(text)\n    text = text.lower()\n\n    # Clean the text\n   # text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=\\(\\)]\", \" \", text) # keep punctuatuin, numnbers and letters\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" . \", text) #Add space to the dot\n    text = re.sub(r\"!\", \" ! \", text) #Add space to the exclamation sign\n    text = re.sub(r\":\", \" :\", text) #Add space before : sign\n    text = re.sub(r\";\", \" ;\", text) #Add space before ; sign\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    #text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    #text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    # find emojis\n    emoji_list = []\n    '''\n    for word in text.split():\n        if any(char in emoji.UNICODE_EMOJI for char in word):\n            emoji_list.append(word)\n    emoji_list'''\n    #text = text.split()\n\n    return text\n\nmy_tok = spacy.load('en')\n#emoji = Emoji(my_tok)\n#my_tok.add_pipe(emoji, first=True)\ndef spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(clean_text(x))]\n\ndef remove_stop_words(tokens): return [tok for tok in tokens if tok not in spacy_stopwords]","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"I'm soooo excited!!!!!This is 10000% the best place on earth:))))) ðŸ˜ƒ...\"","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"\"I'm soooo excited!!!!!This is 10000% the best place on earth:))))) ðŸ˜ƒ...\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text(text)","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"'i am soooo excited ! ! ! ! ! this is 10000% the best place on earth :))))) ðŸ˜ƒ . . . '"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy_tok(clean_text(text))","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"['i',\n 'am',\n 'soooo',\n 'excited',\n '!',\n '!',\n '!',\n '!',\n '!',\n 'this',\n 'is',\n '10000',\n '%',\n 'the',\n 'best',\n 'place',\n 'on',\n 'earth',\n ':',\n ')',\n ')',\n ')',\n ')',\n ')',\n 'ðŸ˜ƒ',\n '.',\n '.',\n '.']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"text2 = \"I also ordered a jade chicken quesadilla on the side.\\n\\nI'm gonna admit, this place looks kinda dirty. I don't think Arizona uses those health department letter grade system like California does, but if I were to just judge by how it looked inside, i'd give it a 'C' grade lol ðŸ˜ƒ\"\n\n\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_stop_words(spacy_tok(clean_text(text2)))","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"['ordered',\n 'jade',\n 'chicken',\n 'quesadilla',\n '.',\n 'gon',\n 'na',\n 'admit',\n 'place',\n 'looks',\n 'kinda',\n 'dirty',\n '.',\n 'think',\n 'arizona',\n 'uses',\n 'health',\n 'department',\n 'letter',\n 'grade',\n 'system',\n 'like',\n 'california',\n 'judge',\n 'looked',\n 'inside',\n 'c',\n 'grade',\n 'lol',\n 'ðŸ˜ƒ']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Building a vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = Counter()\nfor sent in df_reviews['text']:\n    try:\n        counts.update(remove_stop_words(spacy_tok(sent)))\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vocabulary\nvocab2index = {\"\":0, \"UNK\":1}\nwords = [\"\", \"UNK\"]\nfor word in counts:\n    vocab2index[word] = len(words)\n    words.append(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WHat is the 99% quantile of  length of the sentence?\n\ndf_reviews['len_text'] = df_reviews['text'].apply(lambda x: len(x.split()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reviews['len_text'].quantile(0.95)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# note that spacy_tok takes a while run it just once\ndef encode_sentence(sent, vocab2index, N=500, padding_start=True):\n    \"Encoding a sentence adding padding\"\n    x = remove_stop_words(spacy_tok(sent))\n    enc = np.zeros(N, dtype=np.int32)\n    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in x])\n    l = min(N, len(enc1))\n    if padding_start:\n        enc[:l] = enc1[:l]\n    else:\n        enc[N-l:] = enc1[:l]\n    return enc, l","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting into train and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" X_train, X_valid, y_train, y_valid = train_test_split(df_reviews['text'], df_reviews['fun_bin'], test_size=0.33, random_state=42)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.reset_index(inplace=True, drop=True)\nX_valid.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_valid.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Writing a dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class YelpDataset(Dataset):\n    def __init__(self, df, y, N=400, padding_start=True):\n        self.df = df\n        self.X = [encode_sentence(sent, vocab2index, N, padding_start) for sent in self.df]\n        self.y = y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        x, s = self.X[idx]\n        return x, s, self.y[idx]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds =  YelpDataset(X_train, y_train, padding_start=False)\nvalid_ds =  YelpDataset(X_valid, y_valid, padding_start=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg=[]\ni=0\nfor x,s,y in train_ds:\n    if s <=0:\n        neg.append(i)\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(index = 99017, inplace=True)\ny_train.drop(index = 99017, inplace=True)\nX_train.reset_index(inplace=True, drop=True)\ny_train.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds =  YelpDataset(X_train, y_train, padding_start=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg=[]\ni=0\nfor x,s,y in valid_ds:\n    if s <=0:\n        neg.append(i)\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds[100834]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1000\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMV0Model(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super(LSTMV0Model,self).__init__()\n        self.hidden_dim = hidden_dim\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(0.5)\n        \n    def forward(self, x):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        out_pack, (ht, ct) = self.lstm(x)\n        return self.linear(ht[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epocs_v0(model, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x, s, y in train_dl:\n            # s is not used in this model\n            x = x.long().cuda()\n            y = y.float().cuda()\n            y_pred = model(x)\n            optimizer.zero_grad()\n            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss, val_acc = val_metrics_v0(model, valid_dl)\n        if i % 5 == 1:\n            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_metrics_v0(model, valid_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    for x, s, y in valid_dl:\n        # s is not used here\n        x = x.long().cuda()\n        y = y.float().cuda().unsqueeze(1)\n        y_hat = model(x)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        y_pred = y_hat > 0\n        correct += (y_pred.float() == y).float().sum()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss/total, correct/total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(words)\nprint(vocab_size)\nmodel_v0 = LSTMV0Model(vocab_size, 50, 50).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epocs_v0(model_v0, epochs=30, lr=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epocs_v0(model_v0, epochs=30, lr=0.005)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Model with variable length"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset with padding at the end\ntrain_ds_2 =  YelpDataset(X_train, y_train, padding_start=True)\nvalid_ds_2 =  YelpDataset(X_valid, y_valid, padding_start=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMModel(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super(LSTMModel,self).__init__()\n        self.hidden_dim = hidden_dim\n        self.dropout = nn.Dropout(0.5)\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, x, s):\n        # sorting\n        s, sort_index = torch.sort(s.float(), 0,descending=True) # s is the length of the sentence. Sort these lengths\n        s = s.numpy().tolist() # \n        x = x[sort_index]\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        x_pack = pack_padded_sequence(x, s, batch_first=True) # We want LSTM to forget the padding, but in order to apply \n        #ordering mini batches withtin the model\n        out_pack, (ht, ct) = self.lstm(x_pack) \n        out = self.linear(ht[-1]) # Problem here is that output is not sorted! \n        return torch.zeros_like(out).scatter_(0, sort_index.unsqueeze(1).cuda(), out) # scatter_ is undoing the sorting with the given sorting index\n        # kind of sorting back with the original indexing\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epocs(model, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x, s, y in train_dl:\n            x = x.long().cuda()\n            y = y.float().cuda()\n            y_pred = model(x, s)\n            optimizer.zero_grad()\n            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss, val_acc = val_metrics(model, val_dl)\n        if i % 5 == 1:\n            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_metrics(model, val_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    for x, s, y in val_dl:\n        x = x.long().cuda()\n        y = y.float().cuda().unsqueeze(1)\n        y_hat = model(x, s)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        y_pred = y_hat > 0\n        correct += (y_pred.float() == y).float().sum()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss/total, correct/total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 5000\ntrain_dl = DataLoader(train_ds_2, batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(valid_ds_2, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(words)\nprint(vocab_size)\nmodel = LSTMModel(vocab_size, 50, 50).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epocs(model, epochs=50, lr=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epocs(model, epochs=30, lr=0.005)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### CNN with text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"V = vocab_size \nD = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emb = nn.Embedding(vocab_size, 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = emb(x.long())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx1 = x1.transpose(1,2)  # needs to convert x to (batch, embedding_dim, sentence_len)\nx1.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conv_3 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x3 = conv_3(x1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x3.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conv_4 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=4)\nconv_5 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x4 = conv_4(x1)\nx5 = conv_5(x1)\nprint(x4.size(), x5.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 100 3-gram detectors\nx3 = nn.ReLU()(x3)\nx3 = nn.MaxPool1d(kernel_size = 398)(x3)\nx3.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x4 = nn.ReLU()(x4)\nx4 = nn.MaxPool1d(kernel_size = 397)(x4)\nx4.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# 100 5-gram detectors\nx5 = nn.ReLU()(x5)\nx5 = nn.MaxPool1d(kernel_size = 396)(x5)\nx5.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# concatenate x3, x4, x5\nout = torch.cat([x3, x4, x5], 2)\nout.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nout = out.view(out.size(0), -1)\nout.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass CNNModel(nn.Module):\n    \n    def __init__(self, vocab_size, hidden_dim):\n        super(CNNModel, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n    \n        self.conv_3 = nn.Conv1d(in_channels=hidden_dim, out_channels=100, kernel_size=3)\n        self.conv_4 = nn.Conv1d(in_channels=hidden_dim, out_channels=100, kernel_size=4)\n        self.conv_5 = nn.Conv1d(in_channels=hidden_dim, out_channels=100, kernel_size=5)\n        self.dropout = nn.Dropout(p=0.5)\n        self.fc = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        x = x.transpose(1,2)\n        x3 = F.relu(self.conv_3(x))\n        x4 = F.relu(self.conv_4(x))\n        x5 = F.relu(self.conv_5(x))\n        x3 = nn.MaxPool1d(kernel_size = 398)(x3)\n        x4 = nn.MaxPool1d(kernel_size = 397)(x4)\n        x5 = nn.MaxPool1d(kernel_size = 396)(x5)\n        out = torch.cat([x3, x4, x5], 2)\n        out = out.view(out.size(0), -1)\n        out = self.dropout(out)\n        return self.fc(out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(words)\nprint(vocab_size)\nmodel = CNNModel(vocab_size, 300).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing the model\n\nprint(x.shape)\nx = x.long().cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat = model(x)\ny_hat.size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_metrics(m, val_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    for x, s, y in val_dl:\n        x = x.long().cuda()\n        y = y.float().cuda().unsqueeze(1)\n        y_hat = model(x)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        y_pred = y_hat > 0\n        correct += (y_pred.float() == y).float().sum()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss/total, correct/total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epocs(model, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x, s, y in train_dl:\n            x = x.long().cuda()\n            y = y.float().cuda()\n            y_pred = model(x)\n            optimizer.zero_grad()\n            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss, val_acc = val_metrics(model, val_dl)\n        if i % 5 == 1:\n            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 500\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(valid_ds, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CNNModel(vocab_size, 300).cuda()","execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'CNNModel' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e5a4cdd3ffaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'CNNModel' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epocs(model, epochs=30, lr=0.01 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}