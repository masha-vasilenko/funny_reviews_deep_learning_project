{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport string\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n'''from nltk.tokenize import WordPunctTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n'''\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c8b0a68b09ee8fecbbfccdbeb300b63205e542d","trusted":true},"cell_type":"code","source":"yelp_business = pd.read_json('../data/yelp_dataset/business.json', lines=True)\nyelp_business.fillna('NA', inplace=True)\n# we want to make sure we only work with restaurants -- nothing else\nrestaurants = yelp_business[yelp_business['categories'].str.contains('Restaurants')]\nprint('Number of all businesses: ',yelp_business.shape[0])\nprint(f\"Shape of restaurants dataset{restaurants.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yelp_business.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"restaurants.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a6fee5c004788aaff55656025270d0adf7c3747"},"cell_type":"markdown","source":"Now we bring the reviews and perform some preprocessing on those reviews.."},{"metadata":{"_uuid":"508d28c7fdb9065e3dbaf0a39472297fc3b292cf","trusted":false},"cell_type":"code","source":"yelp_review_iter = pd.read_json('../data/yelp_dataset/review.json', chunksize=100000, lines=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6689f58f21411bac18026ba6edfbd0248a8b2f8"},"cell_type":"markdown","source":"Because reviews are too big, we will read them in chunks, and make sure we delete reviews of places that are not in our list of businesses filtered earlier. Note here we choose 5 chunks, but we could have chosen any number (larger numbers will give MemoryError later on)."},{"metadata":{"_uuid":"70007e05b3f8828eb82cc4c4f926f8e657861adb","trusted":false},"cell_type":"code","source":"yelp_review = pd.DataFrame()\ni=0\nfor df in yelp_review_iter:\n    \n    df = df[df['business_id'].isin(restaurants['business_id'])]\n    print(df.shape)\n    yelp_review = pd.concat([yelp_review, df])\n    i=i+1\n    print(i)\n    if i==70: break","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"yelp_review.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85169d7858504ee719c36e795ba46f8eb9a3c909"},"cell_type":"markdown","source":"Also make sure we only get businesses that already show up in our review list and delete the rest."},{"metadata":{"trusted":false},"cell_type":"code","source":"import pickle\nyelp_review.to_pickle(\"pickled_reviews.pickle\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews =pd.read_pickle(\"pickled_reviews.pickle\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1aad8f5fe22db8c3940e07fc3ae49d94000725bf","trusted":false},"cell_type":"code","source":"yelp_business = yelp_business[yelp_business['business_id'].isin(rest_reviews['business_id'])]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6385a3d6f0f21f722925451ecf38ff5c90647b21","trusted":false},"cell_type":"code","source":"print('Final businesses shape: ', yelp_business.shape)\nprint('Final review shape: ', rest_reviews.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews['funny'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.options.display.max_seq_items = 2000\nprint(rest_reviews[rest_reviews['funny']==1290][['business_id', 'text']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews.loc[1331304,'text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check:\nprint( (rest_reviews['funny']>4).mean())\nprint(f\"Number of funny reviews:{(rest_reviews['funny']>4).sum()}\")\n#print(rest_reviews['fun_bin'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"75269/4201684","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews['fun_bin']=rest_reviews['funny'].apply(lambda x: 1 if x>4 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(rest_reviews['fun_bin'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Getting a df with funny reviews"},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_fun = rest_reviews[rest_reviews['fun_bin']==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_fun.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_fun.drop_duplicates(subset= 'text', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_fun.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sampling not funny reviews\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_not_fun = rest_reviews[rest_reviews['fun_bin']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"idx = rest_reviews_not_fun.index.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Balancing the classes: getting the same number of not funny reviews as funny\n#random_hotels = np.random.choice(neg_activity_df[\"hotel\"].unique(), len(neg_activity_df))\nrandom_idx = np.random.choice(idx,rest_reviews_fun.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(random_idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_not_fun = rest_reviews_not_fun.loc[random_idx,:].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rest_reviews_not_fun.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"reviews_final = pd.concat([rest_reviews_fun, rest_reviews_not_fun])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"reviews_final.to_csv(\"../data/yelp_dataset/balanced_reviews.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reading the reviews[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_final = pd.read_csv(\"../input/yelp-reviews/balanced_reviews.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reviews = reviews_final[['funny','text', 'fun_bin']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)\ndf_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data pre-processing"},{"metadata":{},"cell_type":"markdown","source":"#### Goals\n- Keep punctuation\n- Split by \".\", \"!\" to account for misspeling (like \"Hi!I went to...\")\n- Try TF-IDF?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install -U spacy[cuda92]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install spacymoji","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n#import nltk\n#from nltk.corpus import stopwords\n#from nltk.stem import SnowballStemmer\n\nfrom string import punctuation\n\n\nimport numpy as np \nimport pandas as pd \nimport os\nimport spacy\nimport string\nimport re\nimport numpy as np\nfrom spacy.symbols import ORTH\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence \n#from spacymoji import Emoji","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nre_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\ndef sub_br(x): return re_br.sub(\"\\n\", x)\n\n#nlp = spacy.load(\"en\")\nnlp = spacy.load('en_core_web_sm')\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n\nspacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n\ndef clean_text(text):\n    ''' Pre process and convert texts to a list of words '''\n    text = str(text)\n    text = text.lower()\n\n    # Clean the text\n   # text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=\\(\\)]\", \" \", text) # keep punctuatuin, numnbers and letters\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" . \", text) #Add space to the dot\n    text = re.sub(r\"!\", \" ! \", text) #Add space to the exclamation sign\n    text = re.sub(r\":\", \" :\", text) #Add space before : sign\n    text = re.sub(r\";\", \" ;\", text) #Add space before ; sign\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    #text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    #text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    # find emojis\n    emoji_list = []\n    '''\n    for word in text.split():\n        if any(char in emoji.UNICODE_EMOJI for char in word):\n            emoji_list.append(word)\n    emoji_list'''\n    #text = text.split()\n\n    return text\n\nmy_tok = spacy.load('en')\n#emoji = Emoji(my_tok)\n#my_tok.add_pipe(emoji, first=True)\ndef spacy_tok(x): return [tok.text for tok in my_tok.tokenizer(clean_text(x))]\n\ndef remove_stop_words(tokens): return [tok for tok in tokens if tok not in spacy_stopwords]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"I'm soooo excited!!!!!This is 10000% the best place on earth:))))) 😃...\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spacy_tok(clean_text(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text2 = \"I also ordered a jade chicken quesadilla on the side.\\n\\nI'm gonna admit, this place looks kinda dirty. I don't think Arizona uses those health department letter grade system like California does, but if I were to just judge by how it looked inside, i'd give it a 'C' grade lol 😃\"\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_stop_words(spacy_tok(clean_text(text2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building a vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = Counter()\nfor sent in df_reviews['text']:\n    try:\n        counts.update(remove_stop_words(spacy_tok(sent)))\n    except:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vocabulary\nvocab2index = {\"\":0, \"UNK\":1}\nwords = [\"\", \"UNK\"]\nfor word in counts:\n    vocab2index[word] = len(words)\n    words.append(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WHat is the 99% quantile of  length of the sentence?\n\ndf_reviews['len_text'] = df_reviews['text'].apply(lambda x: len(x.split()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reviews['len_text'].quantile(0.95)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# note that spacy_tok takes a while run it just once\ndef encode_sentence(sent, vocab2index, N=500, padding_start=True):\n    \"Encoding a sentence adding padding\"\n    x = remove_stop_words(spacy_tok(sent))\n    enc = np.zeros(N, dtype=np.int32)\n    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in x])\n    l = min(N, len(enc1))\n    if padding_start:\n        enc[:l] = enc1[:l]\n    else:\n        enc[N-l:] = enc1[:l]\n    return enc, l","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting into train and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" X_train, X_valid, y_train, y_valid = train_test_split(df_reviews['text'], df_reviews['fun_bin'], test_size=0.33, random_state=42)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.reset_index(inplace=True, drop=True)\nX_valid.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_valid.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Writing a dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class YelpDataset(Dataset):\n    def __init__(self, df, y, N=400, padding_start=True):\n        self.df = df\n        self.X = [encode_sentence(sent, vocab2index, N, padding_start) for sent in self.df]\n        self.y = y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        x, s = self.X[idx]\n        return x, s, self.y[idx]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds =  YelpDataset(X_train, y_train, padding_start=False)\nvalid_ds =  YelpDataset(X_valid, y_valid, padding_start=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg=[]\ni=0\nfor x,s,y in train_ds:\n    if s <=0:\n        neg.append(i)\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(index = 99017, inplace=True)\ny_train.drop(index = 99017, inplace=True)\nX_train.reset_index(inplace=True, drop=True)\ny_train.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds =  YelpDataset(X_train, y_train, padding_start=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg=[]\ni=0\nfor x,s,y in valid_ds:\n    if s <=0:\n        neg.append(i)\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds[100834]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_train)","execution_count":86,"outputs":[{"output_type":"execute_result","execution_count":86,"data":{"text/plain":"100835"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 1000\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=batch_size)","execution_count":87,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMV0Model(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super(LSTMV0Model,self).__init__()\n        self.hidden_dim = hidden_dim\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(0.5)\n        \n    def forward(self, x):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        out_pack, (ht, ct) = self.lstm(x)\n        return self.linear(ht[-1])","execution_count":88,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epocs_v0(model, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x, s, y in train_dl:\n            # s is not used in this model\n            x = x.long().cuda()\n            y = y.float().cuda()\n            y_pred = model(x)\n            optimizer.zero_grad()\n            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss, val_acc = val_metrics_v0(model, valid_dl)\n        if i % 5 == 1:\n            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))","execution_count":89,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_metrics_v0(model, valid_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    for x, s, y in valid_dl:\n        # s is not used here\n        x = x.long().cuda()\n        y = y.float().cuda().unsqueeze(1)\n        y_hat = model(x)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        y_pred = y_hat > 0\n        correct += (y_pred.float() == y).float().sum()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss/total, correct/total","execution_count":90,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(words)\nprint(vocab_size)\nmodel_v0 = LSTMV0Model(vocab_size, 50, 50).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epocs_v0(model_v0, epochs=30, lr=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epocs_v0(model_v0, epochs=30, lr=0.005)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model with variable length"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset with padding at the end\ntrain_ds_2 =  YelpDataset(X_train, y_train, padding_start=True)\nvalid_ds_2 =  YelpDataset(X_valid, y_valid, padding_start=True)\n","execution_count":91,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMModel(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super(LSTMModel,self).__init__()\n        self.hidden_dim = hidden_dim\n        self.dropout = nn.Dropout(0.5)\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, x, s):\n        # sorting\n        s, sort_index = torch.sort(s.float(), 0,descending=True) # s is the length of the sentence. Sort these lengths\n        s = s.numpy().tolist() # \n        x = x[sort_index]\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        x_pack = pack_padded_sequence(x, s, batch_first=True) # We want LSTM to forget the padding, but in order to apply \n        #ordering mini batches withtin the model\n        out_pack, (ht, ct) = self.lstm(x_pack) \n        out = self.linear(ht[-1]) # Problem here is that output is not sorted! \n        return torch.zeros_like(out).scatter_(0, sort_index.unsqueeze(1).cuda(), out) # scatter_ is undoing the sorting with the given sorting index\n        # kind of sorting back with the original indexing\n    \n    \n    ","execution_count":92,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epocs(model, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x, s, y in train_dl:\n            x = x.long().cuda()\n            y = y.float().cuda()\n            y_pred = model(x, s)\n            optimizer.zero_grad()\n            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss, val_acc = val_metrics(model, val_dl)\n        if i % 5 == 1:\n            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_metrics(model, val_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    for x, s, y in val_dl:\n        x = x.long().cuda()\n        y = y.float().cuda().unsqueeze(1)\n        y_hat = model(x, s)\n        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n        y_pred = y_hat > 0\n        correct += (y_pred.float() == y).float().sum()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss/total, correct/total","execution_count":94,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 5000\ntrain_dl = DataLoader(train_ds_2, batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(valid_ds_2, batch_size=batch_size)","execution_count":95,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(words)\nprint(vocab_size)\nmodel = LSTMModel(vocab_size, 50, 50).cuda()","execution_count":96,"outputs":[{"output_type":"stream","text":"133541\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_epocs(model, epochs=30, lr=0.01)","execution_count":97,"outputs":[{"output_type":"stream","text":"train loss 0.631 val loss 0.655 and val accuracy 0.632\ntrain loss 0.627 val loss 0.606 and val accuracy 0.685\ntrain loss 0.607 val loss 0.582 and val accuracy 0.731\ntrain loss 0.503 val loss 0.549 and val accuracy 0.749\ntrain loss 0.467 val loss 0.551 and val accuracy 0.758\ntrain loss 0.427 val loss 0.541 and val accuracy 0.768\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}